# conf/training/default.yaml
# 파라미터
overwrite_output_dir: True
epochs: 20 # num_train_epochs -> epochs
lr: 1e-5 # learning_rate -> lr
train_batch_size: 30 # per_device_train_batch_size -> train_batch_size
eval_batch_size: 32 # per_device_eval_batch_size -> eval_batch_size
warmup_ratio: 0.1
weight_decay: 0.01
lr_scheduler_type: 'cosine'
optim: 'adamw_torch'
gradient_accumulation_steps: 1
eval_strategy: 'epoch'
save_strategy: 'epoch'
save_total_limit: 5
fp16: True
load_best_model_at_end: True
seed: 42
logging_dir: "./logs"
logging_strategy: "epoch"
predict_with_generate: True
generation_max_length: 100
do_train: True
do_eval: True
early_stopping_patience: 5
early_stopping_threshold: 0.001
report_to: "wandb"

wandb:
  entity: "moonstalker9010-none"
  project: "NLP"
  name: "Moon-baseline"
